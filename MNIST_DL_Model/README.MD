# MNIST Benchmarking: PyTorch vsâ€¯MLX (Singleâ€‘node & Distributed)

<h1>ğŸ§­ Project Overview</h1>
This project compares two simple MNIST classifiers (PyTorch and MLX) on Apple Silicon (MacBook & Mac Mini) to evaluate performance differences in:

1. Model Implementation

2. Training & Evaluation (Singleâ€‘node)

3. Distributed Training (PyTorch)

Everythingâ€™s tracked in detail via metrics: accuracy, inference time, throughput, memory usage, CPU/GPU utilization.


<h1>ğŸ“ Repository Structure </h1>

     MLXProj/
    â”œâ”€â”€ training/
    â”‚   â”œâ”€â”€ train_pytorch.py
    â”‚   â”œâ”€â”€ train_pytorch_ddp.py
    â”‚   â”œâ”€â”€ train_mlx.py
    â”‚â”€â”€ evaluation/
    â”‚   â””â”€â”€ eval_benchmark.py
    |   |__ eval_pytorch.py
    |   |__ eval_benchmark_ddp.py
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ pytorch_model.py
    â”‚   â””â”€â”€ mlx_model.py
    â”œâ”€â”€ results/               # Saved models & logs
    â””â”€â”€ README.md


<h1>ğŸ› ï¸ Environment Setup</h1>

* OS: macOS (MacBookPro (M3 Pro Chip), & 2 Mac Mini's (M4 Pro chips))

* Python: 3.13 virtual environment env_mlx

* Dependencies: torch, torchvision, mlx, psutil, memory_profiler

* Metal/Developer Tools installed (xcode-select)


<h1> ğŸ”„ Singleâ€‘Node Experiments </h1>


<h1> 1. PyTorch (Nonâ€‘MLX) </h1>

* train_pytorch.py trains a simple 2â€‘layer MLP on MNIST

* eval_benchmark.py logs:

* Accuracy (~97.5â€“97.6%)

* Inference time (~0.14â€“0.20â€¯s, ~50â€‘70k samples/sec)

* CPU memory (~300â€¯MB), CPU usage (~2â€“4%), GPU for MLX only

âœ… Trained & evaluated on MacBook & Mac Mini individually

<h1> 2. MLX Model (Singleâ€‘Node) </h1> 

 * train_mlx.py implements equivalent 2â€‘layer MLP with MLX

 * Logs show:

     * Accuracy ~97.4â€“97.8% (comparable to PyTorch)

     * Inference time: ~6â€“8.5â€¯s (1.2â€“1.5k samples/sec; lower throughput)

     * Memory usage: very low (~0.2â€“7â€¯MB)

     * GPU (Metal) used to 100% while running MLX


# ğŸ§µ Multiâ€‘Node (Distributed) Training

# âœ… PyTorch Distributed (DDP)

 * train_pytorch_ddp.py uses torch.distributed + DistributedSampler

 * Launch with mp.spawn(...) (2 nodes, rank 0 & 1)

 * Successfully executed across Mac Minis:

 * Training logs saved per node

    * Performance metrics close to singleâ€‘node speeds

    * Final accuracy ~96â€“97% (expected consistency)


#  ğŸ“Š Performance Summary (So Far)

| Setup                   | Accuracy | Inference Time | Throughput   | Peak Memory (CPU) | Hardware & Notes                                |
|------------------------|----------|----------------|--------------|-------------------|--------------------------------------------------|
| PyTorch (MacBook)      | ~97.6%   | 0.20â€¯s         | ~50â€¯k sps    | ~300â€¯MB           | CPU only                                         |
| PyTorch (Mac Mini)     | ~97.5%   | 0.14â€¯s         | ~68â€¯k sps    | ~311â€¯MB           | CPU only                                         |
| MLX (MacBook)          | ~97.6%   | 7â€“8â€¯s          | ~1.2â€¯k sps   | ~0.3â€“120â€¯MB       | GPU peaks 100% during MLX execution              |
| MLX (Mac Mini)         | ~97.5%   | 6â€“7â€¯s          | ~1.5â€¯k sps   | ~0.1â€“7â€¯MB         | Same GPU behaviour                               |
| PyTorch DDP (Mac Minis)| ~96â€“97%  | ~0.07â€¯s (node) | ~69â€¯k sps    | ~308â€¯MB           | Distributed, per-node metrics available          |

# ğŸ“Œ Whatâ€™s Next
* Implement MLX distributed training using mlx.distributed + hosts.json + MPI
* Run on Mac Mini cluster, monitor similar metrics (time, throughput, memory, CPU/GPU)
* Compare distributed MLX vs. distributed PyTorch vs. single-node results
* Finalize benchmarking report with objective performance insights

# âœ… Conclusion
* PyTorch (nonâ€‘MLX): Fastest inference, simple CPU-only training
* MLX: GPU-utilized Apple-native ML pipeline with lower throughput
* Distributed PyTorch: Achieved multi-node scaling, keeping loss/time consistent
* Distributed MLX: In progressâ€”next milestone to evaluate scalability and performance trade-offs

# ğŸ§­ How to Reproduce
* Clone this repo
* Set up env_mlx, install dependencies (ensure Metal/Xcode tools)
* Run single-node training and evaluation for PyTorch and MLX
* Configure hosts.json and node environments (ulimit, networking)
* Launch PyTorch DDP training
* Implement and launch MLX distributed version
* Analyze results in results/















